{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "mexican-confirmation",
   "metadata": {},
   "source": [
    "# Question 4: General Theory/Application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entitled-cleaners",
   "metadata": {},
   "source": [
    "_No need to be verbose, it's not fun for anyone_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158f1d77",
   "metadata": {},
   "source": [
    "1. What part of S**L**A**M** did this project deal with? Why? What does the other part deal with and how would it generally work, given that you only have LIDAR scans, RGB video stream, and noisy pose data for a moving robot?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a21c03a",
   "metadata": {},
   "source": [
    "SLAM stands for Simultaneous Localization and Mapping, and this project deals with the Localization of the robot. This is because we are concerned with the pose estimates for the robot, i.e, finding out where the robot is and where it has been to estimate a trajectory. The other part of SLAM is Mapping. This is the task of modelling the robot's environment. To build the map of the environment, the SLAM algorithm incrementally processes the lidar scans and builds a pose graph that links these scans. The robot recognizes a previously-visited place through scan matching and may establish one or more loop closures along its path. The optimized scans can be used to build an occupancy map, which represents the environment as a probabilistic occupancy grid. \n",
    "\n",
    "**----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c955f6cf",
   "metadata": {},
   "source": [
    "2. Loop closures play an important role in reducing drift, how would you go about detecting these?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb08339c",
   "metadata": {},
   "source": [
    "Loop closure detecting deals with determining whether a robot has returned to a location that has previously been visited by it. There are several techniques of detecting loop closures, one of which is image to image matching. Images of the environment are captured, and the most recent image is compared with the previously captured images to look for a match. For example, we can use a Bag of Visual Words to compactly describe images and perform similarity queries. It describes certain features of the images as 'visual words', and then compares images by counting how many times these inidividual visual words appear in them.\n",
    "\n",
    "**----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a800bc",
   "metadata": {},
   "source": [
    "3. Explain the structure of your Jacobian. Is the pose-graph fully connected? Why/Why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2b7f15",
   "metadata": {},
   "source": [
    "The Jacobian has number of rows = $3 \\times $ (number of vertices $+$ number of loop closure edges) and number of columns =  $3 \\times $ (number of vertices).\n",
    "\n",
    "\n",
    "Here, number of vertices $= 120$, number of loop closure edges $= 20$. \n",
    "\n",
    "\n",
    "Therefore, the Jacobian has shape $(420, 360)$.\n",
    "\n",
    "The pose graph is not fully connected as every distinct pair of vertices is not connected to each other by an edge. These edges represent spatial constraints between nodes by either odometry information or by revisiting the same location more than once. Fully connected graphs would entail huge amounts of data due to every node having to be connected and lead to erraneous edges and information.\n",
    "\n",
    "**----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "remarkable-hindu",
   "metadata": {},
   "source": [
    "4. With what you know now, how would you describe and differentiate the SLAM frontend and backend? Why do we need to optimise our poses/map in the first place - where does the noise come from/why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac50aa6a",
   "metadata": {},
   "source": [
    "The frotnend part of SLAM processes the sensor data. The raw sensor data is converted into an intermediate representation of constraints between entities (in this case, edges between vertices). The backend takes this intermediate representation and uses it to solve the underlying optimization and update the poses. The sensor data is bound to have certain levels of noise, which causes random incorrect variations of sensor output. The noise will lead to cumulative errors in pose estimation, and such errors will increase with time. Optimization is done to minimize these errors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
